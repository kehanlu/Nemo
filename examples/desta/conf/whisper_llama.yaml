# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: "Kira"
save_dir: null # @kehan custom

trainer:
  devices: [0] # 0 for CPU, or list of the GPUs to use e.g. [0, 1] or [0]
  num_nodes: 1
  max_epochs: 3
  max_steps: -1 # precedence over max_epochs
  accumulate_grad_batches: 1 # accumulates grads every k batches
  gradient_clip_val: 1.0
  precision: "bf16-mixed" #16 # should be set to 16 for O1 and O2 to enable the AMP.
  accelerator: gpu
  log_every_n_steps: 10  # interval of logging.
  val_check_interval: 0.1  # set to 0.25 to check 4 times per epoch, or an int for number of iterations
  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it
  enable_checkpointing: False # provided by exp_manager
  logger: False  # provided by exp_manager
  # strategy: ddp_find_unused_parameters_true
  strategy: ddp

model:
  debug:
    train_log_every_n_steps: 500

  restore_from_path: null
    
  language_model:
    cfg: null # @kehan will overwrite by HF config
    model_id: "meta-llama/Meta-Llama-3-8B-Instruct"
    freeze: True
    # model_id: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

  speech_encoder:
    cfg: null # @kehan will overwrite by HF config
    model_id: "openai/whisper-small"
    freeze: True

  lora: null
  
  connector:
    mode: qformer_1
    prompt_size: 64


  generation_config:
    # max_new_tokens: 128
    max_new_tokens: 80
    do_sample: false
    temperature: null
    top_p: null

  optim:
    name: fused_adam
    lr: 1e-4

    # optimizer arguments
    betas: [0.9, 0.98]
    weight_decay: 0.01

    # scheduler setup
    sched:
      name: CosineAnnealing
      # scheduler params
      warmup_steps: 2000
      # pytorch lightning args
      monitor: val_loss
      reduce_on_plateau: false

exp_manager:
  exp_dir: null  # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
  name: ${name}  # the name of your model
  create_wandb_logger: False
  wandb_logger_kwargs:
    name: ${name}
    project: Kira
  create_tensorboard_logger: False  # whether you want exp_manger to create a tb logger
  resume_if_exists: true # @kehan
  resume_ignore_no_checkpoint: true # @kehan
  create_checkpoint_callback: True
  checkpoint_callback_params:
    monitor: val_loss
    save_top_k: 20
    mode: min
    save_nemo_on_train_end: false
    save_weights_only: False # @kehan, only save state_dict, no optimizer, scheduler...
    filename: '${name}/{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{epoch}'
    always_save_nemo: False
    save_best_model: False
